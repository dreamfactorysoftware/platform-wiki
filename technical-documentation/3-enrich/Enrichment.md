[**HOME**](Home) > [**SNOWPLOW TECHNICAL DOCUMENTATION**](Snowplow technical documentation) > [**Enrichment**](Enrichment) > Enrichment

[[https://d3i6fms1cm1j0i.cloudfront.net/github-wiki/images/3-enrich.png]] 

The Snowplow Enrichment step takes the raw log files generated by the
Snowplow collectors, tidies the data up and enriches it so that it is:

1. Ready to be analysed using EMR
2. Ready to be uploaded into Amazon Redshift, PostgreSQL or some other
alternative storage mechanism for analysis

The current enrichment process provides 2 options for developers to use:

1. Using [Scalding][scalding], a Scala implementation of [Cascading][cascading], an ETL library that's written on top of Hadoop. This is the [Hadoop Enrichment][hadoop-enrich].

Snowplow uses Amazon's EMR to run the Enrichment process. The regular running of the process (which is necessary to ensure that up-to-date Snowplow data is available for analysis) is managed by [EmrEtlRunner][emr-etl-runner], a Ruby application.

2. Using Scala and [Amazon Kinesis][kinesis] for real-time processing
of data.

In this guide, we cover:

1. [The Enrichment Process itself](The-enrichment-process)
2. [How the EmrEtlRunner instruments the regular running of the Enrichment Process][emr-etl-runner]
3. [Scala Kinesis Enrich](scala-kinesis-enrich)


[scalding]: https://github.com/twitter/scalding
[cascading]: http://www.cascading.org/
[kinesis]: http://aws.amazon.com/kinesis/

[emr-etl-runner]: EmrEtlRunner
[hadoop-enrich]: https://github.com/snowplow/snowplow/tree/master/3-enrich/scala-hadoop-enrich
